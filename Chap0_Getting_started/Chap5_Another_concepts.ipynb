{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chap5_Another_concepts.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOFg3AKKV7NeDn1uhYrFyY+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ekvcDnmsqJ-8","colab_type":"text"},"source":["# What does sample, batch, epoch mean?"]},{"cell_type":"markdown","metadata":{"id":"NmSFGX9TqOWa","colab_type":"text"},"source":["Below are common definitions that are necessary to know and understand to correctly utilize keras:  \n","* **Sample**: one element of a dataset.  \n","* **Batch**: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only update to the model.  \n","  - A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).  \n","* **Epoch**: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.  \n","  - When using validation_data or validation_split with the fit method of keras models, evaluation will be run at the end of every epoch.  \n","  - Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an **epoch**. Examples of these are learning rate changes and model checkpointing(saving)."]},{"cell_type":"markdown","metadata":{"id":"B-pAadHAsVeQ","colab_type":"text"},"source":["# How can Save a Keras model?"]},{"cell_type":"markdown","metadata":{"id":"3aXHGnsPsaXn","colab_type":"text"},"source":["**Saving/loading whole models (architeture + weights + optimizer state)**  \n","It is not recommended to use pickle or cPickle to save keras model.  \n","You can use *model.save(filepath)* to save a keras model into a single HDF5 file which will contain:  \n","* The architeture of the model, allowing to re-create the model.  \n","* The weights of the model.  \n","* The training configuration (loss, optimizer).  \n","* The state of the optimizer, allowing to resume training exatly where you left off.  \n","\n","You can then use *keras.models.load_model(filepath)* to reinstantiate your model. Load_model will also take care of compiling the model using the saved training configuration (unless the model was never compiled in the first place).  \n","```python\n","from keras.models import load_model\n","\n","# creates a HDF5 file\n","model.save('my_model.h5')\n","del model # deletes the existing model\n","\n","# returns a compiled model\n","# identical to previous one\n","model = load_model('my_model.h5')\n","```\n","\n","Please also see How can i install HDF5 or h5py to save my models in Keras? For instructions on how to install h5py"]},{"cell_type":"markdown","metadata":{"id":"sjyQ3r25twEi","colab_type":"text"},"source":["**Saving/loading only a model's architeture**  \n","If you only need to save the architecture of a model, and not its weights or its training configuration, you can do:  \n","\n","```python\n","# save as JSON\n","json_string = model.to_json()\n","\n","# save as YAML\n","yaml_string = model.to_yaml()\n","```\n","\n","The generated JSON/YAML files are human-readable and can be manually edited if needed.  \n","\n","You can then build a fresh model from this data:  \n","\n","```python\n","# model reconstruction from JSON\n","from keras.models import model_from_json\n","model = model_from_json(json_string)\n","\n","# model reconstruction from YAML\n","from keras.models import model_from_yaml\n","model = model_from_yaml(yaml_string)\n","```"]},{"cell_type":"markdown","metadata":{"id":"n4ZBoS6bv6Ow","colab_type":"text"},"source":["**Saving/Loading only a model's weights**  \n","If you need to save the weights of a model, you can do so in HDF5 with the code below:  \n","```python\n","model.load_weights('my_model_weights.h5')\n","```\n","\n","Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the same architeture:  \n","```python\n","model.load_weights('my_model_weights.h5', by_name=True)\n","```\n","\n","Example:  \n","```python\n","# Assming the original model looks like this\n","model = Sequential()\n","model.add(Dense(2, input_dim=3, name='dense_1'))\n","model.add(Dense(3, name='dense_2'))\n","...\n","model.save_weights(fname)\n","\n","# new model\n","model = Sequential()\n","model.add(Dense(2, input_dim=3, name='dense_1')) # will be loaded\n","model.add(Dense(10, name='new_dense')) # will not be loaded\n","\n","# load weights from first model, will only affect the first layer, dense_1\n","model.load_weights(fname, by_name=True)\n","```"]},{"cell_type":"markdown","metadata":{"id":"viQKQMGSxgYh","colab_type":"text"},"source":["**Handling custom layers (or other custom objects) in saved models**  \n","If the model you want to load includes custom layers or other custom classes or functions, you can pass them to the loading mechanism via the custom_objects argument:  \n","\n","```python\n","from keras.models import load_model\n","# Assuming your model includes instance of an \"AttentionLayer\" class\n","model = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n","```\n","\n","Alternatively, you can use a custom object scope:  \n","```python\n","from keras.utils import CustomObjectScope\n","\n","with CustomObjectScope({'AttentionLayer': AttentionLayer}):\n","  model = load_model('my_model.h5')\n","```\n","\n","Custom objects handling works the same way for load_model, model_from_json, model_from_yaml:  \n","```python\n","from keras.models import model_from_json\n","model = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer})\n","```"]},{"cell_type":"markdown","metadata":{"id":"RqFZgFBiyrSk","colab_type":"text"},"source":["# Why is the training loss much higher than the testing loss?\n","A keras model has two modes: training and testing. Regularization mechanism, suchs as Dropout and L1/L2 weight regularization, are turned off at testing time.  \n","\n","Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batchesof an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.  \n","\n","[More information](https://www.pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/)"]},{"cell_type":"markdown","metadata":{"id":"v7IeD4-WzmEo","colab_type":"text"},"source":["# How can I obtain the output of an intermediate layer?\n","One simple way is to create a new Model that will output the layers that you are interested in:  \n","```python\n","from keras.models import Model\n","model = .... # create the original model\n","layer_name = 'my_layer'\n","intermediate_layer_model = Model(inputs=model.input,\n","                                outputs=model.get_layer(layer_name).output)\n","intermediate_output = intermediate_layer_model.predict(data)                          \n","```\n","\n","Alternatively, you can build a Keras function that will return the output of a certain layer given a certain input, for example:  \n","```python\n","from keras import backend as k\n","# with a Sequential model\n","get_3rd_layer_output = k.function([model.layers[0].input],\n","                                  [model.layers[3].output])\n","layer_output = get_3rd_layer_output([X])[0]                                  \n","```\n","\n","Similarly, you could build a Theano and TensorFlow function directly.  \n","Note that if your model has a different behavior in training and testing phase (e.g. if it uses Dropout, BatchNormalization, etc), you will need to pass the learning phase flag to your function:  \n","```python\n","get_3rd_layer_output = k.function([model.layers[0].input, k.learning_phase()],\n","                                  [model.lauyers[3].output])\n","# output in test mode = 0\n","layer_output = get_3rd_layer_output([x, 0])[0]\n","# outpuyt in train mode = 1\n","layer_output = get_3rd_layer_output([x, 1])[0]                                  \n","```"]},{"cell_type":"markdown","metadata":{"id":"XcCovsAV6sU7","colab_type":"text"},"source":["# How can I use Keras with datasets that don't fit in memory?\n","You can do batch training using *model.train_on_batch(x, y)* and *model.test_on_batch(x, y)*. See the models documentation.  \n","\n","Alternatively, you can write a generator that yields batches of training data and use the method *model.fit_generator(data_generator, steps_per_epoch, epochs)*."]},{"cell_type":"markdown","metadata":{"id":"GXyMbgkr8iHS","colab_type":"text"},"source":["# How can I interrupt training when the validation loss isn't decreasing anymore?\n","You can use an EarlyStopping callback:  \n","```python\n","from keras.callbacks import EarlyStopping\n","early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n","model.fit(x, y, validation_split=0.2, callbacks=[early_stopping])\n","```"]},{"cell_type":"markdown","metadata":{"id":"-B55SOUT9Mdl","colab_type":"text"},"source":["# How is the validation split computed?\n","If you set the validation_split argument in model.fit to e.g. 0.1, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed.  \n","\n","The same validation set is used for all epochs (within a same call to fit)."]},{"cell_type":"markdown","metadata":{"id":"bp82hKTm9y4P","colab_type":"text"},"source":["# Is the data shuffled during training?\n","Yes, if the shuffle argument in model.fit is set to True (which is the default), the training data will be randomly shuffled at each epoch.  \n","\n","Validation data is never shuffled"]},{"cell_type":"markdown","metadata":{"id":"pu9ejWqk-JOJ","colab_type":"text"},"source":["# How can I record the training / validation loss / accuracy at each epoch?\n","The model.fit method returns a History callback, which has a history attribute the lists of successive losses and other metrics.  \n","```python\n","hist = model.fit(x, y, validation_split=0.2)\n","print(hist.history)\n","```"]},{"cell_type":"markdown","metadata":{"id":"v_3ij5RC-eMP","colab_type":"text"},"source":["# How can I \"freeze\" keras layers?\n","To freeze a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.  \n","\n","You can pass a trainable argument (boolean) to a layer constructor to set a layer to be non-trainable:  \n","```python\n","frozen_layer = Dense(32, trainable=False)\n","```\n","\n","Additionally, you can set the trainable property of a layer to True or False after instantiation. FOr this to take effect, you will need yo call compile() on your model after modifying the trainable property.  \n","```python\n","x = Input(shape=(32,))\n","layer = Dense(32\n","layer.trainable = False\n","y = layer(x)\n","\n","frozen_model = Model(x, y)\n","# in the model below, the weights of layer will not be updated during training\n","frozen_model.compile(optimizer='rmsprop', loss='mse')\n","layer.trainable = True \n","trainable_model = Model(x, y)\n","\n","# with this model the weights of the layer will be updated during training \n","# which will also affect the above model since it uses the same layer instance\n","trainable_model.compile(optimizer='rmsprop', loss='mse')\n","\n","frozen_model.fit(data, labels) # this does NOT update update the weights of layer\n","trainable_model.fit(data, labels) # this updates the weights of layer"]},{"cell_type":"markdown","metadata":{"id":"5vedhJKbL30s","colab_type":"text"},"source":["# How can I remove a layer from a Sequential model?\n","You can remove the last added layer in a Sequential model by calling .pop().  \n","```python\n","model = Sequential()\n","model.add(Dense(32, activation='relu', input_dim=784))\n","model.add(Dense(32, activation='relu'))\n","\n","print(len(model.layers)) # 2\n","model.pop()\n","print(len(model.layers)) # 1\n","```"]},{"cell_type":"markdown","metadata":{"id":"rpGGC5tBNa3L","colab_type":"text"},"source":["# How can I use pre-train models in keras?\n","Code and pre-trained weights are available for the following image classification models:  \n","* Xception  \n","* VGG16  \n","* VGG19  \n","* ResNet  \n","* ResNet v2  \n","* ResNext  \n","* Inception-ResNet v2  \n","* MobileNet v1  \n","* MobileNet v2  \n","* DenseNet  \n","* NASNet  \n","\n","They can be imported from the module keras.applications:  \n","```python\n","from keras.applications.xception import Xception\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg19 import VGG19\n","from keras.applications.resnet import ResNet50\n","from keras.applications.resnet import ResNet101\n","from keras.applications.resnet import ResNet152\n","from keras.applications.resnet_v2 import ResNet50V2\n","from keras.applications.resnet_v2 import ResNet101V2\n","from keras.applications.resnet_V2 import ResNet152V2\n","from keras.applications.resnext import ResNext50\n","from keras.applications.resnext import ResNext101\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.inception_resnet_v2 import InceptionResNetV2\n","from keras.applications.mobilenet import MobileNet\n","from keras.applications.mobilenet_v2 import MobileNetV2\n","from keras.applications.densenet import DenseNet121\n","from keras.applications.densenet import DenseNet169\n","from keras.applications.densenet import DenseNet201\n","from keras.applications.nasnet import NASNetLarge\n","from keras.applications.nasnet import NASNetMobile\n","\n","model = VGG16(weights='imagenet', include_top=True)\n","```\n","\n","For a few simple usage examples, see the documentation for the Applications module.  "]},{"cell_type":"markdown","metadata":{"id":"JI0G8Q4qPIVu","colab_type":"text"},"source":["# How can I use HDF5 inputs with Keras?\n","Youy can use the HDF5Matrix class from keras.utils. See the HDF5Matrix documentation for details.  \n","\n","You can also directly use a HDF5 dataset:  \n","```python\n","import h5py\n","with h5py.File('file.hdf5', 'r') as f:\n","  x_data = f['x_data'])\n","  model.predict(x_data)\n","```"]}]}
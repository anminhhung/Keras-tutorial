{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chap4_Keras_on_GPU.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMGW9yE3ucAq8YJsvgTIvyi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S-u8hkExkicM","colab_type":"text"},"source":["# Run Keras on GPU"]},{"cell_type":"markdown","metadata":{"id":"L11tlHbukmBw","colab_type":"text"},"source":["If you are running on the **TensorFlow** or **CNTK** backends, your code will automatically run on GPU if any avaiable GPU is detectred.  \n","\n","If you are running on the **Theano** backend, you can use one of the following methods:  \n","**Method 1**: use Theano flags  \n","\n","    THEANO_FLAGS = device = gpu, floatX=float32 python my_keras_script.py\n","\n","The name 'gpu' might have to be changed depending on your device's identifier (e.g. gpu0, gpu1, etc).  \n","**Method 2**: set up your .theanorc: Instructions  \n","**Method 3**: manyally set theano.config.device, theano.config.floatX at the beginning of your code:  \n","```python\n","import theano\n","theano.config.device = 'gpu'\n","theano.config.floatX = 'float32'\n","```"]},{"cell_type":"markdown","metadata":{"id":"tggmWTEzlkE1","colab_type":"text"},"source":["# Run a keras model on multiple GPUs?"]},{"cell_type":"markdown","metadata":{"id":"jugqBivHlrio","colab_type":"text"},"source":["I recommend doing so using the **TensorFlow** backend. There are two ways to run a single model on multiple GPUs: **data parallelism** and **device parallelism**.  \n","In most case, what you need is most likely data parallelism."]},{"cell_type":"markdown","metadata":{"id":"VP2CIzskl658","colab_type":"text"},"source":["### Data parallelism"]},{"cell_type":"markdown","metadata":{"id":"i0OUrppwmE0n","colab_type":"text"},"source":["Data parallelism consist in replicating the target model once on each devicem, and using each replace to process a different fraction of the input data. Keras has a built-in utility, *keras.utils.multi_gpu_model*, which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs.  \n","\n","For more information, see the documentation for multi_gpu_model. Here is a quick example:  \n","```python\n","from keras.utils import multi_gpu_model\n","\n","# Replicates `model` on 9 GPUs\n","# This assumes that your machione has 8 available GPUs.\n","parallel_model = multi_gpu_model(model, gpus=8)\n","parallel_model.compile(loss='categorical_crossentropy',\n","                      optimizer='rmsprop')\n","# This `fit` call will be distributed oin 8 GPUs.\n","# Since the batch size is 256, each GPU will process 32 samples\n","parallel_model.fit(x, y, epochs=20, batch_size=256\n","```"]},{"cell_type":"markdown","metadata":{"id":"uTUqg9m_nUhN","colab_type":"text"},"source":["### Device parallelism\n","Device parallelism consist in running different parts of a same model on different devices. It works best for models that have a parallel architeture, e.g. a model with two branches.  \n","\n","This can be achieved by using TensorFlow device scopes.  \n","```python\n","# Model where a shared LSTM is used to encode two different sequences in parallel\n","input_a = keras.Input(shape=(140, 256))\n","input_b = keras.Input(shape=(140, 256))\n","\n","shared_lstm = keras.layers.LSTM(64)\n","\n","# Process the first sequence on one GPU\n","with tf.device_scope('/gpu:0'):\n","  encoded_a = shared_lstm(tweet_a)\n","# Process the next sequence on another GPU\n","with tf.device_scope('/gpu:1'):\n","  encoded_b = shared_lstm(tweet_b)\n","\n","# Concatenate results on CPU\n","with tf.device_scope('/cpu:0'):\n","  merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n","```"]}]}